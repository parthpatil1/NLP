{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmf_topic_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7aki1W-ERUy",
        "colab_type": "text"
      },
      "source": [
        "# 1. Import Libraries\n",
        "\n",
        "Import all the necessary libraries which will be used in this project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQQb4Sd6EVvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "import pyLDAvis\n",
        "from pyLDAvis import sklearn as sklearn_lda \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8XXR7LyEpUe",
        "colab_type": "text"
      },
      "source": [
        "# 2. Load data file\n",
        "\n",
        "Load the data file containing research papers into a dataframe called 'dataset'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynS68zruEmFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data file\n",
        "dataset = pd.read_csv(r'research_papers.csv', encoding='ISO-8859–1')\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgrJoLP2EwZW",
        "colab_type": "text"
      },
      "source": [
        "# 3. Clean Data\n",
        "\n",
        "I dropped the unnecessary columns like 'ID', 'Author','Year', 'Conference/Journal', and focused solely on the 'Abstract' and 'Conclusion' columns of each paper entry. For papers with no conclusions, I filled the empty cell with the text \"No conclusion\". Next, I merged the two columns 'Abstract' and 'Conclusion' to form a new column called 'PaperText'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThgA4VnJEx2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove the unecessary columns\n",
        "dataset = dataset.drop(columns=['Id', 'Reference', 'Codes', 'Authors', 'Year', 'Conference/ Journal'], axis=1)\n",
        "\n",
        "#Fill in the empty cells\n",
        "dataset = dataset.fillna('No conclusion')\n",
        "\n",
        "#Merge abstract and conclusion\n",
        "dataset['Paper_Text'] = dataset[\"Abstract\"] + dataset[\"Conclusion\"]\n",
        "\n",
        "#show first 5 records\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-mDyNKxKqHp",
        "colab_type": "text"
      },
      "source": [
        "# 4. Preprocess Data\n",
        "\n",
        "Tokenize each sentence into a list of words, remove punctuations, remove stopwords and words of length less than 3, and then lemmatize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTFrpOR4xElQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for lemmatization\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "\n",
        "# tokenization\n",
        "tokenized_data = dataset['Paper_Text'].apply(lambda x: x.split()) \n",
        "\n",
        "# Remove punctuation\n",
        "tokenized_data = tokenized_data.apply(lambda x: [re.sub('[-,()\\\\!?]', '', item) for item in x])\n",
        "tokenized_data = tokenized_data.apply(lambda x: [re.sub('[.]', ' ', item) for item in x])\n",
        "\n",
        "#turn to lowercase\n",
        "tokenized_data = tokenized_data.apply(lambda x: [item.lower() for item in x])\n",
        "\n",
        "# remove stop-words and words of length less than 3\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from','use', 'using','uses','user', 'users', 'well', 'study', 'survey', 'think'])\n",
        "tokenized_data = tokenized_data.apply(lambda x: [item for item in x if item not in stop_words and len(item)>3])\n",
        "\n",
        "#lemmatize by calling lemmatization function\n",
        "tokenized_data= tokenized_data.apply(lambda x: [get_lemma(item) for item in x])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqYa5QP0uR12",
        "colab_type": "text"
      },
      "source": [
        "# 5. Create Bigram and Trigram\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
        "\n",
        "Some examples in our corpus are: ‘visually_impaired’, ‘programming_language’, ‘block_based_programming’, 'programming environment' etc.\n",
        "\n",
        "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHX-X6s4uRFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[tokenized_data], threshold=10)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[tokenized_data[0]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8hzwToNmtLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define functions for creating bigrams and trigrams.\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHkVsCQ0cBJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Form Bigrams\n",
        "tokenized_data_bigrams = make_bigrams(tokenized_data)\n",
        "\n",
        " # Form Trigrams\n",
        "tokenized_data_trigrams = make_trigrams(tokenized_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlgYR3IkM0SL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# de-tokenization, combine tokens together\n",
        "detokenized_data = []\n",
        "for i in range(len(dataset)):\n",
        "    t = ' '.join(tokenized_data_trigrams[i])\n",
        "    detokenized_data.append(t)\n",
        "    \n",
        "dataset['clean_text']= detokenized_data \n",
        "documents = dataset['clean_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myuAadxBGPOZ",
        "colab_type": "text"
      },
      "source": [
        "# 6. Perform Exploratory Analysis\n",
        "To verify whether the preprocessing happened correctly, we’ll make a word cloud using the wordcloud package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T72ke8APGPtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(documents.values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_3vzn5pFFx5",
        "colab_type": "text"
      },
      "source": [
        "# 7. Create Document-Term Matrix\n",
        "This is the first step towards topic modeling. We need to represent each and every term and document as a vector.We will use sklearn's TfidfVectorizer to create a document-term matrix using only 1000 terms (words) from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1cE5RqZFIWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set variable number of terms \n",
        "no_terms = 1000\n",
        "\n",
        "# NMF uses the tf-idf count vectorizer\n",
        "# Initialise the count vectorizer with the English stop words\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=no_terms, stop_words='english')\n",
        "# Fit and transform the text\n",
        "document_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "#get features\n",
        "feature_names = vectorizer.get_feature_names()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pcbgL2vCKoS",
        "colab_type": "text"
      },
      "source": [
        "# 8. Apply Topic Model\n",
        "We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn's NMF to perform the task of matrix decomposition. The number of topics can be specified by using the n_components parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QpjZw4GCLZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set variables umber of topics and top words.\n",
        "no_topics = 10\n",
        "no_top_words = 10\n",
        "\n",
        "# Function for displaying topics\n",
        "def display_topic(model, feature_names, num_topics, no_top_words, model_name):    \n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    print(\"Model Result:\")\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-no_top_words - 1:-1]\n",
        "        words = [feature_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i)] = words;\n",
        "    dict = pd.DataFrame(word_dict);\n",
        "    dict.to_csv('%s.csv' % model_name)\n",
        "    return dict\n",
        "\n",
        "# Apply NMF topic model to document-term matrix\n",
        "nmf_model = NMF(n_components=no_topics, random_state=42, alpha=.1, l1_ratio=.5, init='nndsvd').fit(document_matrix)\n",
        "#display topics for nmf model\n",
        "display_topic(nmf_model, feature_names, no_topics, no_top_words, 'NMF_Model_Result')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX5esyBrLgrq",
        "colab_type": "text"
      },
      "source": [
        "# 9. Analyzing our NMF model\n",
        "\n",
        "To analyze the model, we visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which generates an inter-topic distance map. This map is designed to help in the understanding and interpreting of individual topics, and understanding the relationships between the topics. The closer the distance between topics and the more the overlap between topics the worse the performance of the model. \n",
        "\n",
        "Looking at the plot below we can see that for the most part NMF produced topics that had good distance and no overlaps, this means that NMF produces distinctive topics.\n",
        "\t\t\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\t\t\t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5FIUnPnL19t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_prepared = sklearn_lda.prepare(nmf_model, document_matrix, vectorizer)\n",
        "LDAvis_prepared"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dor_Ae83wwkA",
        "colab_type": "text"
      },
      "source": [
        "# 10. Classify papers under topics\n",
        "\n",
        "Using the 10 topics generated by our NMF model, we categorize each paper in our corpus under one of the 10 topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwrBvl3SV2OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use NMF model to assign topic to papers in corpus\n",
        "nmf_topic_values = nmf_model.transform(document_matrix)\n",
        "dataset['NMF Topic'] = nmf_topic_values.argmax(axis=1)\n",
        "\n",
        "#Save dataframe to csv file\n",
        "dataset.to_csv('final_results.csv')\n",
        "dataset.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}